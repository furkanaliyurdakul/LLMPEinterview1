 In this video, I want to start telling you about how we represent neural networks. In other words, how we represent our hypothesis or how we represent our model when using neural networks. Neural networks were developed as simulating neurons or networks of neurons in the brain. So to explain the hypothesis representation, let's start by looking at what a single neuron in the brain looks like. Your brain and mine is jam-packed full of neurons like these, and neurons are cells in the brain. And the two things to draw attention to are that first, the neuron has a cell body like so. And moreover, the neuron has a number of input wires, and these are called the dendrites. You think of them as input wires, and these receive inputs from other locations. And the neuron also has an output wire called the axon. And this output wire is what it uses to send signals to other neurons, or to send messages to other neurons. So at a simplistic level, what a neuron is, is a computational unit that gets a number of inputs through its input wires, does some computation, and then it sends outputs, virus axon, to other nodes, or to other neurons in the brain. Here's an illustration of a group of neurons. The way that neurons communicate with each other is with little pulses of electricity. They're also called spikes, but that just means a little pulse of electricity. So here's one neuron, and what it does is, if it wants to send a message, what it does is it sends a little pulse of electricity via this axon to some different neuron. And here, this axon, that is this output wire, connects to the input wire, connects to the dendrite of this second neuron over here, which then accepts this incoming message, does some computation, and may in turn decide to send out its own messages on its axon to other neurons. And this is the process by which all human thought happens. It's these neurons doing computations and passing messages to other neurons as a result of what other inputs they got. And by the way, this is how our senses and our muscles work as well. If you want to move one of your muscles, the way that works is that a neuron may send these pulses of electricity to your muscle, and that causes your muscles to contract. And your eyes, if some sensor like your eye wants to send a message to your brain, what it does is it sends pulses of electricity to a neuron in your brain like so. In a neural network, or rather in an artificial neural network that we implement in a computer, we're going to use a very simple model of what a neuron does. We're going to model a neuron as just a logistic unit. So when I draw a yellow circle like that, you should think of that as playing a role analogous to maybe the body of a neuron. And we then feed the neuron a few inputs, various dendrites, or its input wires. And the neuron does some computation and outputs some value on its output wire. Or in a biological neuron, this was the axon. And whenever I draw a diagram like this, what this means is that this represents a computation of, you know, h of x equals one over one plus e to the negative theta transpose x. Where, as usual, x and theta are our programs of vectors like so. So this is a very simple, maybe vastly oversimplified model of the computation that the neuron does, where it gets a number of inputs, x1, x2, x3, and it outputs some value computed like so. When I draw a neural network, usually I draw only the input nodes x1, x2, x3. Sometimes, when it's useful to do so, I draw an extra node for x0. This x0 node is sometimes called the bias unit or the bias neuron. But because x0 is always equal to one, sometimes I draw it and sometimes I won't, just depending on whatever is more notationally convenient for that example. Finally, one last bit of terminology. When we talk about neural networks, sometimes we'll say that this is a neuron or an artificial neuron with a sigmoid or a logistic activation function. So this activation function in the neural network terminology, this is just another term for that function, for that non-linearity g of z equals one over one plus e to the negative z. And whereas so far I've been calling theta the parameters of the model, I'll mostly continue to use that terminology to call theta the parameters. But in neural networks, in the neural network literature, sometimes you might hear people talk about weights of a model, and weights just means exactly the same thing as parameters of a model. But I'll mostly continue to use the terminology parameters in these videos. But sometimes you may hear others use the weights terminology. So this little diagram represents a single neuron. What a neural network is, is just a group of these different neurons strung together. Concretely, here we have input units x1, x2, x3. And once again, sometimes we can draw this extra node, x0, and sometimes not. So just draw that in here. And here we have three neurons, which I've written, you know, a21, a22, a23. I'll talk about those indices later. And once again, you know, we can, if we want, add in this a0, and add an extra bias unit there. That always outputs the value of one. And then finally, we have this third node at the final layer. And this is the third node that outputs the value that our hypothesis h of x computes. To introduce a bit more terminology, in a neural network, the first layer, this is also called the input layer, because this is where we input our features, x1, x2, x3. The final layer is also called the output layer, because that layer has the neuron, this one over here, that outputs the final value computed by our hypothesis. And then layer two in between, this is called the hidden layer. The term hidden layer isn't a great terminology, but the intuition is that, you know, in supervised learning, well, you get to see the inputs, you get to see the correct outputs, whereas the hidden layer are values you don't get to observe in the training set. So if it's not x, then it's not y, and so we call those hidden. And later on, we'll see neural networks with more than one hidden layer. But in this example, we have one input layer, layer one, one hidden layer, layer two, and one output layer, layer three. But basically, anything that isn't an input layer and isn't an output layer is called a hidden layer. So I want to be really clear about what this neural network is doing. Let's step through the computational steps that are embodied by this, represented by this diagram. To explain the specific computations represented by a neural network, here's a little bit more notation. I'm going to use a superscript j subscript i to denote the activation of neuron i, or of unit i in layer j. So concretely, this a superscript two subscript one, that's the activation of the first unit in layer two, in our hidden layer. And by activation, I just mean, you know, the value that is computed by and that is output by a specific. In addition, our neural network is parametrized by these matrices, theta superscript j, where theta j is going to be a matrix of weights controlling the function napping from one layer, maybe the first layer to the second layer, or from the second layer to the third layer. So here are the computations that are represented by this diagram. This first hidden unit here has its value computed as follows. A21 is equal to the sigmoid function, or the sigmoid activation function, also called the logistic activation function, applied to this sort of linear combination of its inputs. And then this second hidden unit has this activation value computed as sigmoid of this. And similarly, for this third hidden unit is computed by that formula. So here we have three input units and three hidden units. And so the dimension of theta one, which is the matrix of parameters governing our mapping from our three input units to our three hidden units. Theta one is going to be a three, theta one is going to be a three by four dimensional matrix. And more generally, if a network has sj units in layer j and sj plus one units in layer j plus one, then the matrix theta j, which governs the function mapping from layer j to layer j plus one, that will have dimension sj plus one by sj plus one. Just to be clear about this notation, right? This is s subscript j plus one, and that's s subscript j, and then this whole thing plus one. This whole thing, sj plus one. Okay, so that's s subscript j plus one, plus, by, so that's s subscript j plus one. by sj plus one, by sj plus one, where this plus one is not part of the subscript. Okay, so we've talked about what the three hidden units do to compute their values. Finally, this last, this final, in the output layer, we have one more unit, which computes h of x, and that's equal, can also be written as a three one, and that's equal to this. And you notice that I've written this with a superscript two here, because theta superscript two is the matrix of parameters, or the matrix of weights, that controls the function that maps from the hidden units, that is, the layer two units, to the one layer three unit, that is, the output unit. To summarize, what we've done is shown how a picture like this over here defines an artificial neural network, which defines a function h that maps from x's input values to, hopefully, to some space of predictions y. And these hypotheses are parameterized by parameters that I'm denoting with a capital theta, so that as we vary theta, we get different hypotheses, and we get different functions mapping, say, from x to y. So this gives us a mathematical definition of how to represent the hypothesis in a neural network. In the next few videos, what I'd like to do is give you more intuition about what these hypothesis representations do, as well as go through a few examples and talk about how to compute them efficiently.